#### **为什么采用 Docker 容器部署和微服务架构？**

> 回答 Docker 和微服务的优点，容错隔离，技术栈分离等等。

**容错隔离**：每个微服务运行在独立的容器中，单个服务出错不会影响整体系统，增强系统鲁棒性。

**技术栈分离**：不同微服务可使用不同语言（如 Python、Java）和运行环境，互不影响，提升团队协作效率。

**弹性伸缩**：基于容器的部署便于服务的动态扩展与收缩，应对不同负载需求。

**快速交付**：通过 Docker 镜像构建，支持 CI/CD 持续集成与部署，加快版本迭代。

**易于迁移和测试**：容器具备良好的可移植性，保证开发环境和生产环境一致，避免“在我这能跑”的问题。



#### **作为后端框架，为什么采用 Flask 与 Spring 框架分开？**

本系统采用 **Flask（Python）** 和 **Spring Boot（Java）** 分别构建不同服务模块，主要出于以下考虑：

**职责分离，充分发挥语言特长**：

- Flask 轻量灵活，适用于需要快速开发、密切配合机器学习模型的模块（如模型服务、推理接口、数据处理任务）。
- Spring Boot 具备完善的企业级能力，适用于需要事务管理、权限控制、数据库访问（如 MyBatis）等功能复杂的业务管理模块。

**服务解耦，便于扩展与维护**：

- 各模块通过 API 通信，保持代码独立，避免大工程内部耦合问题。
- 有利于不同开发团队并行工作，提升开发效率。

**生态与性能平衡**：

- Python 拥有丰富的 AI 框架与科学计算库，适合构建模型推理服务。
- Java 的运行性能更优，适合处理高并发业务逻辑与数据访问需求。

**微服务架构支持**：

- 两种框架均可通过容器部署，配合 API 网关（如 Nginx、Traefik）实现统一服务暴露。
- 容器隔离与服务注册发现机制确保语言多样性不影响整体部署一致性。



#### **如何保证平台的可移植性**？

> 回答 Python 和 Java 等开发语言都是解释性语言的特点，同时回答 Pytorch 支持多种计算平台

**使用跨平台语言**：系统核心基于 Python 和 Java 开发，这些语言本身为解释性语言，具有良好的平台无关性，便于在 Windows、Linux、macOS 等主流操作系统上运行。

**支持多平台推理**：采用 PyTorch 框架，其本身原生支持 CPU、GPU（NVIDIA CUDA）、MPS（Apple Silicon）等多种计算平台，确保模型可以无缝迁移至不同硬件设备上运行。

**容器化部署**：结合 Docker 打包环境与依赖，真正做到“一次构建，到处运行”。



#### **如何解决大语言模型在系统集成中存在的信息孤岛**？

> 回答本系统的大语言模型并非简单接入，还包括了知识库与模型上下文协议工具开发

**知识库集成**：结合业务数据库与知识图谱构建私有知识库，通过 RAG（Retrieval-Augmented Generation）方式提升问答准确性。

**上下文协议工具开发**：设计了模型通信协议和上下文管理机制，支持任务状态保存、多轮对话记忆与用户意图识别，提升系统理解力。

**系统级中台接口**：通过统一服务中台接口（API Gateway），将语言模型嵌入平台工作流，避免“信息孤岛”影响用户体验。



#### **工业化部署还有没有什么方案**？

> 回答本项目 API 接口开发完善，可以直接集成和再开发，同时 Docker 容器化等不同部署方案可以实现均衡负载分布式架构等企业级部署方案

**标准化 API 接口**：已完成 RESTful API 的设计与实现，便于外部系统集成与二次开发。

**多种部署模式支持**：

- **Docker 容器化部署**：可快速在任意服务器或云平台上线，支持镜像复用与弹性扩展。
- **企业级集群部署**：可通过 Kubernetes 构建弹性伸缩、高可用的服务集群。
- **负载均衡与多副本策略**：支持 Nginx、HAProxy、Traefik 等中间件，实现高并发场景下的稳定运行。

**如何实现分布式部署**

Docker 均衡负载

|      场景       |       推荐方案       |           核心优势           |
| :-------------: | :------------------: | :--------------------------: |
|  快速原型开发   |  Docker Swarm Mode   |      零配置，分钟级上线      |
| 传统Web应用集群 |  Nginx + docker-gen  |    高定制化，兼容遗留系统    |
|   微服务架构    | Traefik/Consul+Fabio | 动态服务发现，支持金丝雀发布 |
| 大规模生产环境  |  Kubernetes Ingress  |   弹性扩展，完善的监控生态   |
| 物联网边缘计算  | HAProxy + Keepalived |   轻量级，适应资源受限环境   |



#### **可以详细说说这个大模型是如何部署的吗？**

本系统的大语言模型部署采用了**本地部署 + 多语言协同 + 工具调用能力集成**的混合式架构，兼顾了性能、可控性与可扩展性。具体部署如下：

**1. 模型部署方式：基于 Ollama 本地部署 Deepseek 8B**

- 使用 [Ollama](https://ollama.com/) 对 **DeepSeek 8B 模型**进行本地化部署，充分利用本地 GPU 资源，降低延迟、保障数据隐私。
- Ollama 支持原生的 LLM 模型托管与运行环境，兼容性好，便于自定义扩展。

**2. 后端集成方式：基于 Spring Boot + Spring AI 框架**

- 系统后端采用 **Spring AI** 提供的大模型集成框架，通过 `ChatClient` 自动注入 LLM 配置，包括：
  - **RAG 设置**：集成向量库（如 Chroma、PGVector），实现语义检索增强生成（Retrieval-Augmented Generation）。
  - **MCP 客户端设置**：支持与多模态工具或上下文协议协作的插件化配置（MCP = Model Communication Protocol）。
- 所有模型服务通过统一接口封装，后端提供标准 RESTful API，供前端或第三方系统直接调用。

**3. 工具调用机制：Python 开发自定义数据库接口，支持 AI 自主调用**

- 使用 **Python 开发了针对本项目的数据访问与查询工具**，具备以下能力：
  - 查询业务数据库中的火灾案例、设备数据、传感器状态等。
  - 执行预设的数据操作任务，如条件过滤、聚合统计、数据转换等。
- 采用 **标准输入输出（stdio）通信协议** 与 Java 后端连接：
  - Java 作为主进程，通过标准输入向 Python 工具发送请求。
  - Python 执行任务后通过标准输出返回结果，由 AI 再做解析与回复。
  - 这种方式避免了 Web 服务器通信开销，简洁高效，适合工具式调用。

**4. 统一模型服务能力：集成多个功能于一体**

最终部署的大模型系统支持以下核心能力：

- 自然语言问答与多轮对话；
- RAG 知识库问答（集成向量库）；
- 工具函数调用与数据库操作（通过 Python 工具）；
- 多语言模型集成与拓展支持（可后续扩展 Whisper、Stable Diffusion 等模型）；
- 统一接口封装，便于集成至企业系统或 Web 应用。



### **你们团队是如何实现高效分工合作的？**

我们团队采用了**基于 Git 的开发分支流程**与**微服务架构解耦的系统设计**，保证开发过程协同高效、职责明确、冲突可控，具体分工合作方式如下：

我们采用了类似于 Git Flow 的分支策略，明确各成员在不同开发阶段的任务：

| 分支类型          | 功能描述                         | 示例                                           |
| ----------------- | -------------------------------- | ---------------------------------------------- |
| `main` / `master` | 生产环境代码，始终保持稳定       | `main` 分支部署到正式服务器                    |
| `dev`             | 集成测试环境，用于开发合并       | 所有功能开发完成后合入此分支                   |
| `feature/*`       | 功能开发分支，每人负责一项功能   | `feature/pytorch-infer`、`feature/electron-ui` |
| `bugfix/*`        | 问题修复分支，定位并快速修复问题 | `bugfix/login-token`                           |
| `release/*`       | 版本预发布分支，打包测试用       | `release/v1.0.0`                               |



- 每个成员在自己负责的 `feature/*` 分支开发；
- 通过 Pull Request 提交合并请求，统一由组长进行代码审查；
- 使用 GitHub/GitLab 的 Issue 和 Milestone 追踪开发进度与责任人。
